{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tweeter_sample.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. packages"
      ],
      "metadata": {
        "id": "jJCjimtP7Y9k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import re\n",
        "import string \n",
        "\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
        "from sklearn.metrics import confusion_matrix,classification_report, accuracy_score\n",
        "from sklearn.linear_model import LogisticRegression"
      ],
      "metadata": {
        "id": "BLxQm9ut44uC"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldFnjKja4hn4",
        "outputId": "03a11873-2ccf-4cc8-c737-761b9d5889b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n",
            "[nltk_data]   Package twitter_samples is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk \n",
        "nltk.download('twitter_samples')\n",
        "from nltk.corpus import twitter_samples\n",
        "from nltk.corpus import stopwords          # module for stop words that come with NLTK\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem import PorterStemmer        # module for stemming\n",
        "from nltk.tokenize import TweetTokenizer   # module for tokenizing strings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. prepare data"
      ],
      "metadata": {
        "id": "iunjgx1U7d5U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twitter_samples.fileids()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oYvfU-ZO4rak",
        "outputId": "b9dec906-9296-46fa-f972-c90ae1a36541"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# documents\n",
        "docs_negative = [(t, \"neg\") for t in twitter_samples.strings(\"negative_tweets.json\")]\n",
        "docs_positive = [(t, \"pos\") for t in twitter_samples.strings(\"positive_tweets.json\")]\n",
        "print(f'There are {len(docs_negative)} negative sentences.')\n",
        "print(f'There are {len(docs_positive)} positive sentences.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pbZIEVM7gL5",
        "outputId": "a20d82c5-df89-46a0-968e-1b51adcdbedd"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 5000 negative sentences\n",
            "There are 5000 positive sentences\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spliting dataset \n",
        "train_set = docs_negative[:3500] + docs_positive[:3500]\n",
        "test_set = docs_negative[3500:4250] + docs_positive[3500:4250]\n",
        "valid_set = docs_negative[4250:] + docs_positive[4250:]"
      ],
      "metadata": {
        "id": "A4DqUiXg9BO1"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clean text\n",
        "def process_text(text):\n",
        "    stemmer = PorterStemmer()\n",
        "    stopwords_english = stopwords.words('english')\n",
        "    #text = text.str\n",
        "    text = re.sub(r'\\$\\w*', '', text)\n",
        "    text = re.sub(r'^RT[\\s]+', '', text)\n",
        "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text)\n",
        "    text = re.sub(r'#', '', text)\n",
        "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True,reduce_len=True)\n",
        "    text_tokens = tokenizer.tokenize(text)\n",
        "\n",
        "    text_clean = []\n",
        "    for word in text_tokens:\n",
        "        if (word not in stopwords_english and  \n",
        "                word not in string.punctuation): \n",
        "            stem_word = stemmer.stem(word)  # stemming word\n",
        "            text_clean.append(stem_word)\n",
        "            \n",
        "    sentence = ' '.join(text_clean)\n",
        "    \n",
        "    return sentence"
      ],
      "metadata": {
        "id": "oXbyNau15TTA"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def xy(dataset):\n",
        "  df = pd.DataFrame(dataset, columns = ['text', 'label'])\n",
        "  df['text_clean'] = df['text'].apply(lambda r: process_text(r))\n",
        "  df['categorical_label'] = df.label.factorize()[0]\n",
        "\n",
        "  x = df.text_clean\n",
        "  y = df.categorical_label\n",
        "\n",
        "  return x, y"
      ],
      "metadata": {
        "id": "7I_iyxV4-PHV"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# dataframe\n",
        "x_train, y_train = xy(train_set)\n",
        "x_test, y_test = xy(test_set)\n",
        "x_valid, y_valid = xy(valid_set)"
      ],
      "metadata": {
        "id": "bcokYxZq9x3-"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes"
      ],
      "metadata": {
        "id": "65xcO6gbG_pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pipeline([\n",
        "    ('bow',CountVectorizer()),  # strings to token integer counts\n",
        "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
        "    ('classifier', MultinomialNB()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
        "])"
      ],
      "metadata": {
        "id": "EoqWq5DDAIQs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F592LZaKAITK",
        "outputId": "8e655e13-4b6d-44ee-fe97-aa121a1bd111"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('bow', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('classifier', MultinomialNB())])"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "print(confusion_matrix(y_pred,y_test))\n",
        "print(classification_report(y_pred,y_test))\n",
        "print(accuracy_score(y_pred,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WcROg1lpAIVd",
        "outputId": "14bc1d59-902c-48a2-fe40-4402f57acad5"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[593 235]\n",
            " [157 515]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.72      0.75       828\n",
            "           1       0.69      0.77      0.72       672\n",
            "\n",
            "    accuracy                           0.74      1500\n",
            "   macro avg       0.74      0.74      0.74      1500\n",
            "weighted avg       0.74      0.74      0.74      1500\n",
            "\n",
            "0.7386666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic"
      ],
      "metadata": {
        "id": "jQIdzQEXHCCs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Pipeline([\n",
        "    ('bow',CountVectorizer()),  # strings to token integer counts\n",
        "    ('tfidf', TfidfTransformer()),  # integer counts to weighted TF-IDF scores\n",
        "    ('classifier', LogisticRegression()),  # train on TF-IDF vectors w/ Naive Bayes classifier\n",
        "])"
      ],
      "metadata": {
        "id": "kXK4HFRZHEEJ"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "768da9a2-ca7c-4c69-9ca2-6860cd8d3aee",
        "id": "eRu8WfeaHEEK"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(steps=[('bow', CountVectorizer()), ('tfidf', TfidfTransformer()),\n",
              "                ('classifier', LogisticRegression())])"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = model.predict(x_test)\n",
        "print(confusion_matrix(y_pred,y_test))\n",
        "print(classification_report(y_pred,y_test))\n",
        "print(accuracy_score(y_pred,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64b656d6-a2f6-4bbe-fd9d-2d897383eeaa",
        "id": "4xMI0_UgHEEL"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[590 233]\n",
            " [160 517]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.72      0.75       823\n",
            "           1       0.69      0.76      0.72       677\n",
            "\n",
            "    accuracy                           0.74      1500\n",
            "   macro avg       0.74      0.74      0.74      1500\n",
            "weighted avg       0.74      0.74      0.74      1500\n",
            "\n",
            "0.738\n"
          ]
        }
      ]
    }
  ]
}